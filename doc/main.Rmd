---
title: "Fried chicken or dogs or blueberry muffins"
author: "Fall2017-project3-fall2017-project3-grp4"
date: "10/30/2017"
output:
  pdf_document: default
  html_document: default
---

# Summary

In this project, we created a classification engine for images of dogs versus fried chicken versus blueberry muffins. We tried classifiers (GBM, Logistic Regression, SVM, Random Forest and Neural Networks) under different feature extraction(Sift, LBP, HoG). By comparing the accurancy rate as well as the processing time, we finally chose the best classification method.

# Step 0. Install, Load packages 

Because of using the package caret, when we trained model we also perform cross validation by using the `tuneGrid` and `traincontrol` arguement, so we ignore creating cross validation R file.

```{r, warning=F, message=FALSE}
packages.used=c("caret","gbm", "e1071", "DMwR", "nnet", "randomForest","OpenImageR","DT", "caTools", "EBImage", "mxnet", "pbapply", "ggthemes")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}

library(caret)
library(gbm)
library(e1071)
library(DMwR)
library(randomForest)
library(nnet)
library(OpenImageR)
library(DT)
library(caTools)
library(EBImage)
library(mxnet)
library(pbapply)
library(ggthemes)

source("../lib/train.R")
source("../lib/test.R")

```

# Step 1. Construct Baseline Model

# Step 1.1. Load Feature
 
 We devided the whole training set into 'df_train' & 'df_test'
 
 'df_train' is the training data (80%)                               
 
 'df_test' is the testing data (20%) 
 
```{r, eval=FALSE}
source("../lib/train_test_split.R")
df = train_test_split("sift") 
Sift_train = df$df_train
Sift_test = df$df_test
```

# Step 1.2. Training Process of Baseline model
 
```{r, eval = FALSE}
baseline.result = train_gbm(Sift_train)
```

```{r}

load("../output/baseline.result.Rdata")
baseline.time = baseline.result[[4]]
baseline.time

baseline.accuracy = 1 - baseline.result[[3]]
print(paste0("The accuracy rate of baseline model is ", baseline.accuracy))
```

# Step 1.3. Predict result
 
 We used GBM, Logistic Regression, SVM, Random Forest to construct the classification process
 separatly. However, the process really need a long time.
  
```{r}
# Classifications                   Accuracy Rate
# Sift + GBM                              ~73%
# Sift + Logistic Regression              ~71%
# Sift + Random Forest                    ~73%
# Sift + SVM                              ~33%
```

# Step 2. Construct Visual Features

The feature Sift, which has 5000 diamensions, is extremely over-weighted, espacially using classifier SVM. Thus we need to explore some other features. In the following process, we extracted LBP features and HoG features.

# Step 2.1. Local Binary Patterns (LBP)
 
* Divide the examined window into cells (e.g. 16x16 pixels for each cell).

* For each pixel in a cell, compare the pixel to each of its 8 neighbors 2

*  Where the center pixel is value is greater than the neighbors value, write 0. Otherwise, write 1 
*  This gives an 8-digit binary number.

* Compute the histogram over the cell. This histogram can be seen as a 256-dimensional feature
vector.

 
# Step 2.1.1. Creat and Load Feature
 We used matlab to extract LBP feature, and finally produce 555 diamensions.
 
```{r}

df = train_test_split("lbp") 
LBP_train = df$df_train
LBP_test = df$df_test

```

# Step 2.1.2. Training Process under LBP
 
# LBP + Classifiers

```{r, eval=FALSE}

# LBP + Random Forest
rf.fit.LBP = train_rf(LBP_train)

# LBP + Logistic Regresstion
lr.fit.LBP = train_lr(LBP_train)

# LBP + SVM
svm.fit.LBP = train_svm(LBP_train)

# LBP + GBM
gbm.fit.LBP = train_gbm(LBP_train)

```
# Step 2.1.3. Result table of LBP extraction

```{r, eval=FALSE}

source("../lib/train_test_accuracy.R")
lbp.result = train_test_accuracy("lbp")

```

```{r}

load("../output/lbp.result.Rdata")

lbp.result

```

# Step 2.2. HoG
 
 We used R to extract HoG feature, and finally produce 251 diamensions, and the parameter of HoG  should be 5 and 10.
 
```{r }

load("../output/hog.para.accuracy.Rdata")
hog.para.accuracy

```

 
# Step 2.2.1. Load Feature
 
```{r}

df = train_test_split("hog510") 
HoG_train = df$df_train
HoG_test = df$df_test

```

# Step 2.2.2. Training Process under HoG

# HoG + Classifiers

```{r, eval=FALSE}
# HoG + Random Forest
rf.fit.HoG = train_rf(HoG_train)

# HoG + Logistic Regresstion
lr.fit.HoG = train_lr(HoG_train)

# HoG + SVM
SVM.fit.HoG = train_rf(SVM.fit.HoG)

# HoG + GBM
gbm.fit.HoG = train_gbm(HoG_train)
```

# Step 2.2.3. Result table of HoG extraction

```{r, eval=FALSE}

source("../lib/train_test_accuracy.R")
hog.result = train_test_accuracy("hog510")

```

```{r}

load("../output/hog.result.Rdata")

hog.result
```

# Step 2.3. Neural Networks

For Neural Networks, we use the raw image data to derectly train and test the model.

# Step 2.3.1 Neural Networks Feature Extraction
```{r, eval=FALSE}
source('../lib/cnn_feature.R')
cnn_feature = extract_feature(img_dir)

```

# Step 2.3.2 Neural Networks Model Training
```{r, eval=FALSE}
cnn = train_cnn(cnn_feature)
save(cnn, "../output/cnn.fit.Rdata")
```

# Step 2.3.2 Neural Networks Model Training
```{r}
load(file = '../output/cnn.test.Rdata')
df_cnn
```

 
# Step 3. Model Selection

```{r, echo=FALSE}

lbp.result$Feature<- 'LBP'
hog.result$Feature<- 'HoG'
comp<-rbind(lbp.result,hog.result)
comp<-comp[,c(1,5,2,3,4)]

par(mfrow=c(1,2))

ggplot(comp, aes(x=Model,y=Test_accuracy,color=Feature,fill=Feature)) +
  geom_bar(stat="identity", position=position_dodge())+
  theme_solarized(light = T)+
   ylim(0, 1.2)

ggplot(comp, aes(x=Model,y=Running_Time,color=Feature,group=Feature)) +
  geom_line()+
  theme_solarized(light = T)

```


```{r,echo=FALSE}

comp<-rbind(comp,c("GBM","Sift",1-baseline.result[[2]],1-baseline.result[[3]],baseline.result[[4]]))
comp<-rbind(comp,c("CNN","NULL",1,1,1))
comp$Parameters<-c("Ntree=","Multinomial","Cost= , Margin= " , "GBM",
                   "Ntree=","Multinomial","Cost= , Margin= " , "GBM",
                   "gbm","Layer=")
library(DT)
datatable(comp)

```

# Step 4. Final Test
Depending on our analysis, finally we choose LBP with classifier Logistic Regresstion as our main method. We also want to test the result using others classification method, here are the results.


# Step. 4.1. Feature Extraction

```{r}
# load("../output/lbp_test.csv")
# output name df_test
```

# Step. 4.2. Test Process under LBP

```{r, eval=FALSE}
load("../output/lr.fit.LBP.Rdata")
load("../output/rf.fit.LBP.Rdata")
load("../output/svm.fit.LBP.Rdata")
load("../output/gbm.fit.LBP.Rdata")

test_lr_accuracy = mean(test(lr, df_test))
test_rf_accuracy = mean(test(rf, df_test))
test_svm_accuracy = mean(test(svm, df_test))
test_gbm_accuracy = mean(test(gbm, df_test))

```

# Step 4.3. Final Analysis Table

```{r, echo=F, eval=FALSE}

df_result_final = data.frame(Model = c('Logistic Regresstion', 'Random Forest', 'SVM', 'gbm') , Accuracy = c(test_lr_accuracy,test_rf_accuracy,test_svm_accuracy,test_gbm_accuracy), Time = c(lr$time, rf$time, svm$time, gbm$time))

datatable(Accuracy)

```



